{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AakRte18To4U"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain faiss-cpu pypdf docx2txt chromadb tiktoken gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import openai"
      ],
      "metadata": {
        "id": "mnoimkYKUUw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Your API Key\""
      ],
      "metadata": {
        "id": "rPiMl8O3UbFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 0\n",
        "MODEL_NAME = \"gpt-3.5-turbo\" #gpt-4\n",
        "class docqa_agent(object):\n",
        "  def __init__(self,\n",
        "               open_ai_api_key,\n",
        "               model=MODEL_NAME,\n",
        "               temperature=TEMPERATURE):\n",
        "    openai.api_key = open_ai_api_key\n",
        "    self._openai_key  = open_ai_api_key\n",
        "    self.chat_model = ChatOpenAI(model=model, temperature=temperature)\n",
        "    self.qa = None\n",
        "  def ingest(self,doc_path):\n",
        "    '''\n",
        "    Ingest the split text to vector DB\n",
        "    '''\n",
        "\n",
        "    texts  = self.load_data(doc_path)\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=self._openai_key)\n",
        "    # load it into Chroma\n",
        "    db = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "    return db\n",
        "  def load_data(self,doc_path):\n",
        "\n",
        "      # Load all the files in a folder\n",
        "      loader =  Docx2txtLoader(doc_path)# , glob=\"**/*.md\")\n",
        "      documents = loader.load()\n",
        "      text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
        "      texts = text_splitter.split_documents(documents)\n",
        "      return texts\n",
        "  def make_qa(self,doc_path):\n",
        "    self.db = self.ingest(doc_path.name) # .name to get path\n",
        "    self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=MODEL_NAME), chain_type=\"stuff\", retriever=self.db.as_retriever(search_kwargs={\"k\": 3}))\n",
        "    print('QA Chain is sucessfully made.')\n",
        "  def run(self,query):\n",
        "    if self.qa is not None:\n",
        "      answer = self.qa.run(query)\n",
        "      return answer\n",
        "    else:\n",
        "      print('Please make qa chain first.')"
      ],
      "metadata": {
        "id": "dgJQzHFtlT9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = docqa_agent(os.environ[\"OPENAI_API_KEY\"] )"
      ],
      "metadata": {
        "id": "CttD8_wfb-Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "app = gr.Blocks()\n",
        "with app:\n",
        "  gr.Markdown(\"## Q&A over your docx documents\")\n",
        "  with gr.Tabs():\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        query = gr.Textbox(\n",
        "                            EXAMPLE_QUERY, label=\"Question of your document\", lines=2)\n",
        "        docs_upload = gr.File(\n",
        "                            file_count=\"single\", file_types=[\".docx\"])\n",
        "        ans_button = gr.Button(\"Submit\")\n",
        "      with gr.Column():\n",
        "        answer = gr.Textbox(\n",
        "                            value=\"Your answer will appear here\",\n",
        "                            label=\"Answer\",\n",
        "                            lines=5,\n",
        "                        )\n",
        "  docs_upload.upload(\n",
        "      qa.make_qa,\n",
        "      inputs = docs_upload\n",
        "  )\n",
        "  ans_button.click(\n",
        "      qa.run,\n",
        "      inputs =[query],\n",
        "      outputs = [answer]\n",
        "  )\n",
        "app.launch(share=True,debug=False)"
      ],
      "metadata": {
        "id": "pe6rSeRvb-IK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "f017fa9c-e568-48df-9211-772ce00826dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://edb1dd531ba95175d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://edb1dd531ba95175d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.__dir"
      ],
      "metadata": {
        "id": "Hp0mt0r4e03k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe21c20-efa1-4a84-8b65-43c761f8b0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stfQojSJeaoX",
        "outputId": "1a5e2a6b-7dd6-43e4-b9ce-b8f2c0aa23c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    }
  ]
}